{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tiny_imagenet.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "TojIUERqm3zH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.misc import imsave\n",
        "import scipy.ndimage as nd\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import h5py\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from keras import backend as K\n",
        "import scipy.ndimage as nd\n",
        "from keras.models import Sequential,load_model\n",
        "from keras import regularizers\n",
        "from keras import initializers\n",
        "from keras import metrics\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, Convolution2D, MaxPooling2D, BatchNormalization, ReLU, LeakyReLU \n",
        "from keras import layers\n",
        "from keras import models\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "niwtd0kMg3tK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mh1lDXXo6anU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# download TinyImageNet\n",
        "! git clone https://github.com/seshuad/IMagenet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RtGJxCzirJL6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_id_dictionary():\n",
        "    id_dict = {}\n",
        "    for i, line in enumerate(open('IMagenet/tiny-imagenet-200/wnids.txt', 'r')):\n",
        "        id_dict[line.replace('\\n', '')] = i\n",
        "    return id_dict\n",
        "  \n",
        "def get_class_to_id_dict():\n",
        "    id_dict = get_id_dictionary()\n",
        "    all_classes = {}\n",
        "    result = {}\n",
        "    for i, line in enumerate(open('IMagenet/tiny-imagenet-200/words.txt', 'r')):\n",
        "        n_id, word = line.split('\\t')[:2]\n",
        "        all_classes[n_id] = word\n",
        "    for key, value in id_dict.items():\n",
        "        result[value] = (key, all_classes[key])\n",
        "        \n",
        "    return result\n",
        "\n",
        "def get_data(id_dict):\n",
        "\n",
        "    print('starting loading data')\n",
        "    train_data, val_data, test_data = [], [], []\n",
        "    train_labels, val_labels, test_labels = [], [], []\n",
        "    t = time.time()\n",
        "    for key, value in id_dict.items():\n",
        "        train_data += [nd.imread('IMagenet/tiny-imagenet-200/train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), mode='RGB') for i in range(450)]\n",
        "        \n",
        "        train_labels_ = np.array([[0]*200]*450)\n",
        "        train_labels_[:, value] = 1\n",
        "        train_labels += train_labels_.tolist()\n",
        "        \n",
        "        val_data += [nd.imread('IMagenet/tiny-imagenet-200/train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), mode='RGB') for i in range(450, 500)]\n",
        "        \n",
        "        val_labels_ = np.array([[0]*200]*50)\n",
        "        val_labels_[:, value] = 1\n",
        "        val_labels += val_labels_.tolist()\n",
        "\n",
        "    for line in open('IMagenet/tiny-imagenet-200/val/val_annotations.txt'):\n",
        "        img_name, class_id = line.split('\\t')[:2]\n",
        "        test_data.append(nd.imread('IMagenet/tiny-imagenet-200/val/images/{}'.format(img_name), mode='RGB'))\n",
        "\n",
        "        test_labels_ = np.array([[0]*200])\n",
        "        test_labels_[0, id_dict[class_id]] = 1\n",
        "        test_labels += test_labels_.tolist()\n",
        "\n",
        "    print('finished loading data, in {} seconds'.format(time.time() - t))\n",
        "\n",
        "    return np.array(train_data), np.array(train_labels), np.array(val_data), np.array(val_labels), np.array(test_data), np.array(test_labels)\n",
        "  \n",
        "def make_augmented_train(id_dict):\n",
        "    if not(os.path.isdir('drive/My Drive/tiny-imagenet-200-A')):\n",
        "        print('making dir')\n",
        "        os.mkdir('drive/My Drive/tiny-imagenet-200-A')\n",
        "        os.mkdir('drive/My Drive/tiny-imagenet-200-A/train')\n",
        "        os.mkdir('drive/My Drive/tiny-imagenet-200-A/test')\n",
        "  \n",
        "    crops = [((4, 60), (4, 60)),\n",
        "             ((0, 56), (0, 56)),\n",
        "             ((8, 64), (8, 64)),\n",
        "             ((0, 56), (8, 64)),\n",
        "             ((8, 64), (0, 56))]\n",
        "    \n",
        "    trans = [((4, 60), (4, 60)),\n",
        "             ((8, 64), (8, 64)),\n",
        "             ((0, 56), (0, 56)),\n",
        "             ((8, 64), (0, 56)),\n",
        "             ((0, 56), (8, 64))]\n",
        "    \n",
        "    print('starting creating augmeneted data')\n",
        "    train_data, train_labels = [], []\n",
        "    t = time.time()\n",
        "    \n",
        "    progress = 0\n",
        "    for key, value in id_dict.items():\n",
        "        print(progress)\n",
        "        progress += 1\n",
        "        if not(os.path.isdir('drive/My Drive/tiny-imagenet-200-A/train/{}'.format(key))):\n",
        "            os.mkdir('drive/My Drive/tiny-imagenet-200-A/train/{}'.format(key))\n",
        "            os.mkdir('drive/My Drive/tiny-imagenet-200-A/train/{}/images'.format(key))\n",
        "        count = 0\n",
        "        for crp, trs in zip(crops, trans):\n",
        "            crop_x, crop_y = crp\n",
        "            tran_x, tran_y = trs\n",
        "            for mirror in [-1, 1]:\n",
        "                for i in range(450):\n",
        "                    img = nd.imread('IMagenet/tiny-imagenet-200/train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), mode='RGB')[:, ::mirror][crop_x[0]:crop_x[1], crop_y[0]:crop_y[1]]\n",
        "                    processed_img = np.zeros(shape = (64, 64, 3))\n",
        "                    processed_img[tran_x[0]:tran_x[1], tran_y[0]:tran_y[1]] = img\n",
        "                    \n",
        "                    imsave('drive/My Drive/tiny-imagenet-200-A/train/{}/images/{}_{}.JPEG'.format(key, key, str(count)), processed_img)\n",
        "                    count += 1\n",
        "\n",
        "    print('finished creating augmeneted data, in {} seconds'.format(time.time() - t))\n",
        "  \n",
        "def shuffle_data(train_data, train_labels, val_data, val_labels):\n",
        "    size = len(train_data)\n",
        "    train_idx = np.arange(size)\n",
        "    np.random.shuffle(train_idx)\n",
        "    \n",
        "    size = len(val_data)\n",
        "    val_idx = np.arange(size)\n",
        "    np.random.shuffle(val_idx)\n",
        "\n",
        "    return train_data[train_idx], train_labels[train_idx], val_data[val_idx], val_labels[val_idx]\n",
        "  \n",
        "def write_results(model, comments, train_history, filename):\n",
        "    file = open('drive/My Drive/results/' + filename, 'w')\n",
        "    acc, loss, val_acc, val_loss = train_history['acc'], train_history['loss'], train_history['val_acc'], train_history['val_loss']\n",
        "    train_top5, valid_top5 = train_history['top_k_categorical_accuracy'], train_history['val_top_k_categorical_accuracy']\n",
        "    \n",
        "    model.summary(print_fn=lambda x: file.write(x + '\\n'))\n",
        "    file.write(comments + '\\n')\n",
        "    \n",
        "    for metric in acc, loss, train_top5, val_acc, val_loss, valid_top5:\n",
        "        file.write('*')\n",
        "        for value in metric:\n",
        "            file.write(str(value) + ' ')\n",
        "        file.write('\\n')\n",
        "        \n",
        "    file.close()\n",
        "    \n",
        "def Dense_reg(width, activation, l2, init):\n",
        "    if l2 == 0: return Dense(width, activation=activation, kernel_initializer=init)\n",
        "    else: return Dense(width, activation=activation, kernel_regularizer=regularizers.l2(l2), kernel_initializer=init)\n",
        "    \n",
        "\n",
        "def cnn_1(dropout = 0.0, l2 = 0):\n",
        "    model = Sequential ()\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', activation='relu', strides=(2,2), input_shape=(64, 64, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Convolution2D(128, (5, 5), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Convolution2D(256, (3, 3), padding='same', activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense_reg(256, 'relu', l2, init))\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense_reg(256, 'relu', l2, init))\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense(200, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "  \n",
        "  \n",
        "def cnn_2(dropout = 0.0, l2 = 0.0):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', strides=(2,2), input_shape=(64, 64, 3)))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Convolution2D(128, (5, 5), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(128, (5, 5), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Convolution2D(256, (3, 3), padding='same'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense_reg(256, None, l2))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense_reg(256, None, l2))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense(200, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "  \n",
        "def cnn_3(dropout = 0.0, l2 = 0.0):\n",
        "    model = Sequential()\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', strides=(2,2), input_shape=(64, 64, 3), kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(128, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(128, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(256, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(256, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense_reg(512, None, l2, init))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense_reg(512, None, l2, init))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense(200, activation='softmax', kernel_initializer=init))\n",
        "\n",
        "    return model\n",
        "  \n",
        "def cnn_4(dropout = 0.0, l2 = 0.0):\n",
        "    model = Sequential()\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', strides=(2,2), input_shape=(64, 64, 3), kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(128, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(128, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(256, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(256, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense_reg(1024, None, l2, init))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense_reg(1024, None, l2, init))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense(200, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "    \n",
        "def cnn_5(dropout = 0.0, l2 = 0.0):\n",
        "    model = Sequential()\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "    \n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', strides=(2,2), input_shape=(64, 64, 3), kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(256, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(256, (5, 5), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(512, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(512, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "    model.add(Convolution2D(512, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(512, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Convolution2D(512, (3, 3), padding='same', kernel_initializer=init))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense_reg(1024, None, l2, init))\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense_reg(1024, None, l2, init))\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense(200, activation='softmax'))\n",
        "\n",
        "    return model\n",
        "    \n",
        "def resnet_baseline(x, dropout):\n",
        "    \n",
        "    def residual_block(y, nb_channels, kernel_size):\n",
        "\n",
        "        shortcut = y\n",
        "\n",
        "        y = layers.Conv2D(nb_channels, kernel_size=kernel_size, strides=(1, 1), padding='same')(y)\n",
        "        y = LeakyReLU()(y)\n",
        "        y = layers.BatchNormalization()(y)\n",
        "\n",
        "        y = layers.Conv2D(nb_channels, kernel_size=kernel_size, strides=(1, 1), padding='same')(y)\n",
        "        \n",
        "        y = layers.add([shortcut, y])\n",
        "        y = LeakyReLU()(y)\n",
        "        y = layers.BatchNormalization()(y)\n",
        "\n",
        "        return y\n",
        "    \n",
        "    x = layers.Conv2D(48, kernel_size=(7, 7), strides=(2, 2), padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv2D(96, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 96, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 96, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 96, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(192, kernel_size=(3, 3), padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 192, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 192, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 192, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(2048, activation = 'relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(2048, activation = 'relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(200, activation = 'softmax')(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "def resnet_stanford(x, dropout):\n",
        "  \n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(7, 7), strides=(2, 2), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = residual_block(x, 64, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 64, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 64, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 64, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv2D(128, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 128, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 128, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 128, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 256, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 256, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 256, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(512, kernel_size=(3, 3), strides=(2, 2), padding='same', activation='relu')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 512, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 512, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = residual_block(x, 512, (3, 3))\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, activation = 'relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(512, activation = 'relu')(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Dense(200, activation = 'softmax')(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "def residual_block(y, nb_channels, kernel_size, init):\n",
        "\n",
        "    shortcut = y\n",
        "\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_initializer=init)(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    y = layers.Conv2D(nb_channels, kernel_size=kernel_size, strides=(1, 1), padding='same', kernel_initializer=init)(y)\n",
        "    y = layers.BatchNormalization()(y)\n",
        "\n",
        "    y = layers.add([shortcut, y])\n",
        "    y = layers.BatchNormalization()(y)\n",
        "    y = layers.LeakyReLU()(y)\n",
        "\n",
        "    return y\n",
        "  \n",
        "  \n",
        "def resnet_1(x):\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 64, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(128, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 128, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 256, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, kernel_initializer=init)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dense(512, kernel_initializer=init)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dense(200, activation = 'softmax')(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "def resnet_2(x):\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 64, (5, 5), init)\n",
        "    x = residual_block(x, 64, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(128, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 128, (5, 5), init)\n",
        "    x = residual_block(x, 128, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 256, (5, 5), init)\n",
        "    x = residual_block(x, 256, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(512, kernel_size=(3, 3), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 512, (3, 3), init)\n",
        "    x = residual_block(x, 512, (3, 3), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(512, kernel_initializer=init)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dense(512, kernel_initializer=init)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dense(200, activation = 'softmax')(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "def resnet_3(x):\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(5, 5), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 64, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(128, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 128, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 256, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1024, kernel_initializer=init)(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dense(1024, kernel_initializer=init)(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dense(200, activation = 'softmax')(x)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "def resnet_4(x):\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(3, 3), strides=(2, 2), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    \n",
        "    x = layers.Conv2D(64, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 64, (5, 5), init)\n",
        "    x = residual_block(x, 64, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(128, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 128, (5, 5), init)\n",
        "    x = residual_block(x, 128, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    \n",
        "    x = layers.Conv2D(256, kernel_size=(5, 5), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 256, (5, 5), init)\n",
        "    x = residual_block(x, 256, (5, 5), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    \n",
        "    x = layers.Conv2D(512, kernel_size=(3, 3), padding='same', kernel_initializer=init)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = residual_block(x, 512, (3, 3), init)\n",
        "    x = residual_block(x, 512, (3, 3), init)\n",
        "    x = layers.MaxPool2D(pool_size=(2, 2), padding='same')(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1024, kernel_initializer=init)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.Dense(1024, kernel_initializer=init)(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    x = layers.Dense(200, activation = 'softmax')(x)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "  \n",
        "def cnn_1_leaky(dropout = 0.0, l2 = 0):\n",
        "    model = Sequential ()\n",
        "    init = initializers.glorot_normal(seed=0)\n",
        "\n",
        "    model.add(Convolution2D(64, (5, 5), padding='same', strides=(2,2), input_shape=(64, 64, 3)))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Convolution2D(128, (5, 5), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Convolution2D(256, (3, 3), padding='same'))\n",
        "    model.add(LeakyReLU())\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    \n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense_reg(256, None, l2, init))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense_reg(256, None, l2, init))\n",
        "    model.add(LeakyReLU())\n",
        "    if dropout > 0: model.add(Dropout(dropout))\n",
        "    model.add(Dense(200, activation='softmax'))\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0qSYJuavSxRJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data, train_labels, val_data, val_labels, test_data, test_labels = get_data(get_id_dictionary())\n",
        "train_data, train_labels, val_data, val_labels = shuffle_data(train_data, train_labels, val_data, val_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WSzYnCDi9EqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# example run\n",
        "model = cnn_1()\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "train_history = model.fit(train_data, train_labels, batch_size=1000, epochs=30, validation_data=(val_data, val_labels))\n",
        "\n",
        "write_results(model,  \"no regularisation, no augmentation\", train_history, 'cnn_1.txt')\n",
        "del model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}